{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "reguser = pd.read_csv('/mnt/datasets/fusai/user_register_log.txt', sep='\\t',\n",
    "                    names=['uid','rday','rtype','dtype'],\n",
    "                    dtype={0: np.uint32, 1: np.uint8, 2: np.uint8, 3: np.uint16})\n",
    "applog = pd.read_csv('/mnt/datasets/fusai/app_launch_log.txt', sep='\\t',\n",
    "                  names=['uid','logday'],\n",
    "                  dtype={0: np.uint32, 1: np.uint8})\n",
    "vidlog = pd.read_csv('/mnt/datasets/fusai/video_create_log.txt', sep='\\t',\n",
    "                  names=['uid','pday'],\n",
    "                  dtype={0: np.uint32, 1: np.uint8})\n",
    "useract = pd.read_csv('/mnt/datasets/fusai/user_activity_log.txt', sep='\\t',\n",
    "                    names=['uid','aday','page','vid','aid','atype'],\n",
    "                    dtype={0: np.uint32, 1: np.uint8, 2: np.uint8, 3: np.uint32, 4: np.uint32, 5: np.uint8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prereg(reguser,d1,d2):\n",
    "    #n号之前注册的用户\n",
    "    reguser1 = reguser.loc[reguser.rday <= d2]\n",
    "    #注册日期与当前的差距\n",
    "    reguser1['rday'] = d2 + 1 - reguser1['rday']\n",
    "    #多值特征\n",
    "    for i in range(0,6):\n",
    "        reguser2 = reguser1.loc[reguser1.rtype == i]\n",
    "        reguser2 = reguser2.groupby('uid')['rtype'].size().reset_index().rename(columns = {0:'rtype_' + str(i)})\n",
    "        reguser1 = pd.merge(reguser1,reguser2,on = 'uid', how = 'left')\n",
    "    \n",
    "    #dtpye onehot\n",
    "    for i in range(0,20):\n",
    "        reguser2 = reguser1.loc[reguser1.dtype == i]\n",
    "        reguser2 = reguser2.groupby('uid')['rtype'].size().reset_index().rename(columns = {0:'dtype_' + str(i)})\n",
    "        reguser1 = pd.merge(reguser1,reguser2,on = 'uid', how = 'left')\n",
    "    \n",
    "    #用户拍摄视频数\n",
    "    useract1 = useract.loc[(useract.aday >= d1) & (useract.aday <= d2),['uid','vid','aid']]\n",
    "    useract2 = useract1.loc[:,['aid','vid']]\n",
    "    useract2.drop_duplicates(inplace=True)\n",
    "    useract2 = useract2.groupby(['aid'])['vid'].size().reset_index().rename(columns = {0:'vidcnts','aid':'uid'})\n",
    "    reguser1 = pd.merge(reguser1,useract2,on = 'uid',how = 'left')\n",
    "    \n",
    "    useract2 = useract1.loc[:,['aid','uid']]\n",
    "    useract2.drop_duplicates(inplace=True)\n",
    "    useract2 = useract2.groupby(['aid'])['uid'].size().reset_index().rename(columns = {0:'watch','aid':'uid'})\n",
    "    reguser1 = pd.merge(reguser1,useract2,on = 'uid',how = 'left')\n",
    "    \n",
    "    reguser1['ave_watch'] = reguser1['watch']/reguser1['vidcnts']\n",
    "    \n",
    "    reguser2 = reguser1.groupby('dtype')['uid'].size().reset_index().rename(columns = {0:'dtypecnt'})\n",
    "    reguser1 = pd.merge(reguser1,reguser2,on = 'dtype',how = 'left')\n",
    "    \n",
    "    del reguser2\n",
    "    reguser1 = reguser1.fillna(value=0)\n",
    "    print 'reg done'\n",
    "    return reguser1\n",
    "\n",
    "import itertools\n",
    "def test1(row):\n",
    "    t = np.diff(row)\n",
    "    temp_list = [0]\n",
    "    if(len(t)):\n",
    "        for k,v in itertools.groupby(t):\n",
    "            if k == 1:\n",
    "                temp_list.append(len(list(v)))\n",
    "        num = max(temp_list) + 1\n",
    "        return num\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def preapp(d1,d2):\n",
    "    applog1 = applog.loc[(applog.logday <= d2)]#(applog.logday >= d1) & \n",
    "    #登陆日期调整到相对日期\n",
    "    applog1['logday'] = applog1['logday'] - d1\n",
    "    \n",
    "    #最大连续登陆\n",
    "    applog1 = applog1.sort_index(axis = 0,ascending = True,by = ['uid','logday'])\n",
    "    applog2 = applog1.loc[:,['uid']]\n",
    "    applog2.drop_duplicates(inplace=True)\n",
    "    applog2 = applog2.reset_index(drop = True)\n",
    "    c = pd.DataFrame({'a':applog1.groupby(by = 'uid',as_index=False).agg({'logday':test1})['logday']})\n",
    "    applog2['maxlog'] = c['a']\n",
    "    applog1 = pd.merge(applog1,applog2,on = 'uid',how = 'right')\n",
    "    \n",
    "    #最近登陆\n",
    "    applog2 = applog1.groupby('uid')['logday'].max().reset_index().rename(columns = {'logday': 'lastlog'})\n",
    "    applog1 = pd.merge(applog1,applog2,on = 'uid',how = 'right')\n",
    "    \n",
    "    applog1['log_gap'] = d2 - d1 + 1 - applog1['lastlog']\n",
    "    \n",
    "    #7天内总登陆次数\n",
    "    applog2 = applog1.groupby('uid')['logday'].count().reset_index().rename(columns = {'logday': 'logsum'})\n",
    "    applog1 = pd.merge(applog1,applog2,on = 'uid',how = 'right')\n",
    "    #多值特征\n",
    "    for i in range(0,d2-d1+1):\n",
    "        applog2 = applog1.loc[applog1.logday == i]\n",
    "        applog2 = applog2.groupby('uid')['logday'].count().reset_index().rename(columns = {'logday':'log'+ str(i) + 'day'})\n",
    "        applog1 = pd.merge(applog1,applog2,on = 'uid',how = 'left')\n",
    "    del applog2\n",
    "    \n",
    "    applog1.pop('logday')\n",
    "    applog1.drop_duplicates(inplace=True)\n",
    "    applog1 = applog1.fillna(value=0)\n",
    "    print 'app done'\n",
    "    return applog1\n",
    "\n",
    "def previd(d1,d2):\n",
    "    vidlog1 = vidlog.loc[(vidlog.pday <= d2)]#(vidlog.pday >= d1) & \n",
    "    vidlog1['pday'] = vidlog1['pday'] - d1\n",
    "    \n",
    "    #最大连续拍摄\n",
    "    vidlog2 = vidlog1.sort_index(axis = 0,ascending = True,by = ['uid','pday'])\n",
    "    vidlog2.drop_duplicates(inplace=True)\n",
    "    vidlog3 = vidlog2.loc[:,['uid']]\n",
    "    vidlog3.drop_duplicates(inplace=True)\n",
    "    vidlog3 = vidlog3.reset_index(drop = True)\n",
    "    c = pd.DataFrame({'a':vidlog2.groupby(by = 'uid',as_index=False).agg({'pday':test1})['pday']})\n",
    "    vidlog3['maxp'] = c['a']\n",
    "    vidlog1 = pd.merge(vidlog1,vidlog3,on = 'uid',how = 'right')\n",
    "    \n",
    "    #最多拍摄\n",
    "    vidlog2 = vidlog1.groupby(['uid','pday'])['maxp'].count().reset_index().rename(columns = {'maxp':'pdaysum'})\n",
    "    vidlog2 = vidlog2.groupby('uid')['pdaysum'].max().reset_index().rename(columns = {'pdaysum':'mostp'})\n",
    "    vidlog1 = pd.merge(vidlog1,vidlog2,on = 'uid',how = 'right')\n",
    "    \n",
    "    #用户拍摄的总天数\n",
    "    vidlog2 = vidlog1.loc[:,['uid','pday']]\n",
    "    vidlog2.drop_duplicates(inplace=True)\n",
    "    vidlog2 = vidlog2.groupby(['uid'])['pday'].size().reset_index().rename(columns = {0:'pdaycnt'})\n",
    "    vidlog1 = pd.merge(vidlog1,vidlog2,on = 'uid',how = 'right')\n",
    "    \n",
    "    #最近拍摄\n",
    "    vidlog2 = vidlog1.groupby('uid')['pday'].max().reset_index().rename(columns = {'pday': 'lastp'})\n",
    "    vidlog1 = pd.merge(vidlog1,vidlog2,on = 'uid',how = 'right')\n",
    "\n",
    "    vidlog1['pday_gap'] = d2 - d1 + 1 - vidlog1['lastp']\n",
    "\n",
    "    #总拍摄次数\n",
    "    vidlog2 = vidlog1.groupby('uid')['pday'].count().reset_index().rename(columns = {'pday': 'sump'})\n",
    "    vidlog1 = pd.merge(vidlog1,vidlog2,on = 'uid',how = 'right')\n",
    "    #多值特征\n",
    "    for i in range(0,d2-d1+1):\n",
    "        vidlog2 = vidlog1.loc[vidlog1.pday == i]\n",
    "        vidlog2 = vidlog2.groupby('uid')['pday'].size().reset_index().rename(columns = {0:'p'+ str(i) + 'day'})\n",
    "        vidlog1 = pd.merge(vidlog1,vidlog2,on = 'uid',how = 'left')\n",
    "    del vidlog2\n",
    "    \n",
    "    #平均每天拍摄\n",
    "    vidlog1['ave_p'] = vidlog1['sump']/vidlog1['pdaycnt']\n",
    "    \n",
    "    vidlog1.pop('pday')\n",
    "    vidlog1 = vidlog1.fillna(value=0)\n",
    "    vidlog1.drop_duplicates(inplace=True)\n",
    "    \n",
    "    print 'vid done'\n",
    "    return vidlog1\n",
    "\n",
    "def preact(d1,d2):\n",
    "    useract1 = useract.loc[(useract.aday <= d2)]#(useract.aday >= d1) & \n",
    "    useract1['aday'] = useract1['aday'] - d1\n",
    "    \n",
    "    #最近操作时间\n",
    "    useract2 = useract1.groupby('uid')['aday'].max().reset_index().rename(columns = {'aday': 'lasta'})\n",
    "    useract1 = pd.merge(useract1,useract2,on = 'uid',how = 'right')\n",
    "    useract1['aday_gap'] = d2 - d1 + 1 - useract1['lasta']\n",
    "    \n",
    "    del useract1['vid']\n",
    "    del useract1['aid']\n",
    "    del useract2\n",
    "    del useract1['aday']\n",
    "    del useract1['page']\n",
    "    del useract1['atype']\n",
    "    useract1.drop_duplicates(inplace=True)\n",
    "    useract1 = useract1.fillna(value=0)\n",
    "    print 'act done'\n",
    "    return useract1\n",
    "\n",
    "def atype_cnt(d1,d2,i):\n",
    "    useract1 = useract.loc[(useract.aday <= d2),['uid','atype']]#(useract.aday >= d1) & \n",
    "    \n",
    "    useract2 = useract1.loc[useract1.atype == i,['uid','atype']]\n",
    "    del useract1\n",
    "    useract2 = useract2.groupby(['uid'])['atype'].count().reset_index().rename(columns = {'atype':'atype'+str(i)+'cnt'})\n",
    "    useract2.drop_duplicates(inplace=True)\n",
    "    useract2 = useract2.fillna(value=0)\n",
    "    print '用户各种type总数'\n",
    "    return useract2\n",
    "\n",
    "def page_cnt(d1,d2,i):\n",
    "    useract1 = useract.loc[(useract.aday <= d2),['uid','page']]#(useract.aday >= d1) & \n",
    "    useract2 = useract1.loc[useract1.page == i,['uid','page']]\n",
    "    del useract1\n",
    "    useract2 = useract2.groupby(['uid'])['page'].count().reset_index().rename(columns = {'page':'page'+str(i)+'cnt'})\n",
    "    useract2.drop_duplicates(inplace=True)\n",
    "    useract2 = useract2.fillna(value=0)\n",
    "    print '不同page下总操作数'\n",
    "    return useract2\n",
    "\n",
    "def aidhot(d1,d2):\n",
    "    useract1 = useract.loc[(useract.aday <= d2),['uid','vid','aid']]#(useract.aday >= d1) & \n",
    "    \n",
    "    useract2 = useract1.groupby(['aid'])['vid'].size().reset_index().rename(columns = {0:'aidcnt'})\n",
    "    useract1 = pd.merge(useract1,useract2,on = 'aid',how = 'left')  \n",
    "    useract2 = useract1.groupby(['uid'])['aidcnt'].mean().reset_index().rename(columns = {'aidcnt':'aidhot'})\n",
    "    useract2.drop_duplicates(inplace=True)\n",
    "    useract2 = useract2.fillna(value=0)\n",
    "    print 'aidhot done'\n",
    "    return useract2\n",
    "\n",
    "def vidhot(d1,d2):\n",
    "    useract1 = useract.loc[(useract.aday <= d2),['uid','vid','aid']]#(useract.aday >= d1) & \n",
    "    \n",
    "    useract2 = useract1.groupby(['vid'])['aid'].size().reset_index().rename(columns = {0:'vidcnt'})\n",
    "    useract1 = pd.merge(useract1,useract2,on = 'vid',how = 'left')\n",
    "    useract2 = useract1.groupby(['uid'])['vidcnt'].mean().reset_index().rename(columns = {'vidcnt':'vidhot'})\n",
    "    useract2.drop_duplicates(inplace=True)\n",
    "    useract2 = useract2.fillna(value=0)\n",
    "    print 'vidhot done'\n",
    "    return useract2\n",
    "    \n",
    "def ave_act(d1,d2):\n",
    "    useract1 = useract.loc[(useract.aday <= d2),['uid','aday']]#(useract.aday >= d1) & \n",
    "    useract1['aday'] = useract1['aday'] - d1\n",
    "    \n",
    "    #用户总操作数\n",
    "    useract2 = useract1.groupby(['uid'])['aday'].size().reset_index().rename(columns = {0:'act_sum'})\n",
    "    useract1 = pd.merge(useract1,useract2,on = ['uid'],how = 'right')\n",
    "    \n",
    "    #用户每天平均操作数\n",
    "    useract2 = useract1.groupby('uid')['aday'].min().reset_index().rename(columns = {'aday': 'firsta'})\n",
    "    useract1 = pd.merge(useract1,useract2,on = 'uid',how = 'right')\n",
    "    useract1['first_gap'] = d2 - d1 + 1 - useract1['firsta']\n",
    "    useract1['ave_act'] = useract1['act_sum']/useract1['first_gap']\n",
    "    del useract1['aday']\n",
    "    useract1.drop_duplicates(inplace=True)\n",
    "    useract1 = useract1.fillna(value=0)\n",
    "    print 'ave_act done'\n",
    "    return useract1\n",
    "\n",
    "def adaycnt(d1,d2):\n",
    "    useract1 = useract.loc[(useract.aday <= d2),['uid','aday']]#(useract.aday >= d1) & \n",
    "    #用户有act的总天数\n",
    "    useract1.drop_duplicates(inplace=True)\n",
    "    useract1 = useract1.groupby(['uid'])['aday'].size().reset_index().rename(columns = {0:'adaycnt'})\n",
    "    useract1.drop_duplicates(inplace=True)\n",
    "    useract1 = useract1.fillna(value=0)\n",
    "    print '用户有act的总天数'\n",
    "    return useract1\n",
    "\n",
    "def asum_n(d1,d2):\n",
    "    useract1 = useract.loc[(useract.aday <= d2),['uid','aday']]#(useract.aday >= d1) & \n",
    "    useract1['aday'] = useract1['aday'] - d1\n",
    "    \n",
    "    #用户每天操作数\n",
    "    useract2 = useract1.groupby(['uid','aday']).size().reset_index().rename(columns = {0:'asum'})\n",
    "    useract2['asum'] = (np.log(useract2['asum'] + 1)/np.log(2)).astype(int)\n",
    "    for i in range(0,d2-d1+1):\n",
    "        useract_temp = useract2.loc[useract2.aday == i,['uid','asum']]\n",
    "        useract_temp['asum_' + str(i)] = useract_temp['asum']\n",
    "        del useract_temp['asum']\n",
    "        useract2 = pd.merge(useract2,useract_temp,on = 'uid',how = 'left')\n",
    "    del useract_temp\n",
    "    del useract2['asum']\n",
    "    del useract2['aday']\n",
    "    del useract1\n",
    "    useract2.drop_duplicates(inplace=True)\n",
    "    useract2 = useract2.fillna(value=0)\n",
    "    print 'act_sum_n done'\n",
    "    return useract2\n",
    "\n",
    "def addlabel(d1,d2):\n",
    "    applog1 = applog.loc[(applog.logday >= d1) & (applog.logday <= d2)]\n",
    "    vidlog1 = vidlog.loc[(vidlog.pday >= d1) & (vidlog.pday <= d2)]\n",
    "    useract1 = useract.loc[(useract.aday >= d1) & (useract.aday <= d2)]\n",
    "    user = pd.concat([applog1['uid'],vidlog1['uid'],useract1['uid']])\n",
    "    del applog1\n",
    "    del vidlog1\n",
    "    del useract1\n",
    "    user.drop_duplicates(inplace=True)\n",
    "    user = pd.DataFrame({'uid':user})\n",
    "    user['label'] = 1\n",
    "    user = user.fillna(value=0)\n",
    "    return user\n",
    "\n",
    "def predata(d1,d2):\n",
    "    test = prereg(reguser,d1,d2)\n",
    "    \n",
    "    # for i in range(0,6):\n",
    "    test = pd.merge(test,atype_cnt(d1,d2,0),on = 'uid',how = 'left')\n",
    "    # for i in range(0,5):\n",
    "    test = pd.merge(test,page_cnt(d1,d2,0),on = 'uid',how = 'left')\n",
    "    \n",
    "    test = pd.merge(test,preapp(d1,d2),on = 'uid',how = 'left')\n",
    "    \n",
    "    \n",
    "    test = pd.merge(test,atype_cnt(d1,d2,1),on = 'uid',how = 'left')\n",
    "    test = pd.merge(test,page_cnt(d1,d2,1),on = 'uid',how = 'left')\n",
    "    \n",
    "    test = pd.merge(test,previd(d1,d2),on = 'uid',how = 'left')\n",
    "    \n",
    "    test = pd.merge(test,atype_cnt(d1,d2,2),on = 'uid',how = 'left')\n",
    "    test = pd.merge(test,page_cnt(d1,d2,2),on = 'uid',how = 'left')\n",
    "\n",
    "    test = pd.merge(test,preact(d1,d2),on = 'uid',how = 'left')\n",
    "    \n",
    "    test = pd.merge(test,atype_cnt(d1,d2,3),on = 'uid',how = 'left')\n",
    "    test = pd.merge(test,page_cnt(d1,d2,3),on = 'uid',how = 'left')\n",
    "    test = pd.merge(test,atype_cnt(d1,d2,4),on = 'uid',how = 'left')\n",
    "    test = pd.merge(test,page_cnt(d1,d2,4),on = 'uid',how = 'left')\n",
    "    test = pd.merge(test,atype_cnt(d1,d2,5),on = 'uid',how = 'left')\n",
    "    \n",
    "    test = pd.merge(test,asum_n(d1,d2),on = 'uid',how = 'left')\n",
    "    # test = pd.merge(test,aidhot(d1,d2),on = 'uid',how = 'left')\n",
    "    # test = pd.merge(test,vidhot(d1,d2),on = 'uid',how = 'left')\n",
    "    test = pd.merge(test,ave_act(d1,d2),on = 'uid',how = 'left')\n",
    "    test = pd.merge(test,adaycnt(d1,d2),on = 'uid',how = 'left')\n",
    "    # print test.columns\n",
    "    \n",
    "    test['ave_act_1'] = test['act_sum']/test['adaycnt']\n",
    "    \n",
    "    test['asum1'] = test['asum_9'] + test['asum_10'] * 2 + test['asum_11'] * 3 + test['asum_12'] * 4\\\n",
    "                + test['asum_13'] * 5 + test['asum_14'] * 6 + test['asum_15'] * 7\n",
    "    test['psum1'] = test['p9day'] + test['p10day'] * 2 + test['p11day'] * 3 + test['p12day'] * 4\\\n",
    "                + test['p13day'] * 5 + test['p14day'] * 6 + test['p15day'] * 7\n",
    "                \n",
    "    test['atype0_ratio'] = test['atype0cnt']/test['act_sum']\n",
    "    test['atype1_ratio'] = test['atype1cnt']/test['act_sum']\n",
    "    test['atype2_ratio'] = test['atype2cnt']/test['act_sum']\n",
    "    test['atype3_ratio'] = test['atype3cnt']/test['act_sum']\n",
    "    test['atype4_ratio'] = test['atype4cnt']/test['act_sum']\n",
    "    test['atype5_ratio'] = test['atype5cnt']/test['act_sum']\n",
    "    \n",
    "    test['page0_ratio'] = test['page0cnt']/test['act_sum']\n",
    "    test['page1_ratio'] = test['page1cnt']/test['act_sum']\n",
    "    test['page2_ratio'] = test['page2cnt']/test['act_sum']\n",
    "    test['page3_ratio'] = test['page3cnt']/test['act_sum']\n",
    "    test['page4_ratio'] = test['page4cnt']/test['act_sum']\n",
    "    \n",
    "    test.drop_duplicates(inplace=True)\n",
    "    test = test.fillna(value=0)\n",
    "    return test\n",
    "\n",
    "def pretrain():\n",
    "    train = predata(8,23)\n",
    "    train = pd.merge(train,addlabel(24,30),on = 'uid',how = 'left')\n",
    "    \n",
    "    temp = predata(1,16)\n",
    "    temp = pd.merge(temp,addlabel(17,23),on = 'uid',how = 'left')\n",
    "    train = pd.concat([train,temp])\n",
    "    \n",
    "    del temp\n",
    "    \n",
    "    train.drop_duplicates(inplace=True)\n",
    "    train['label'] = train['label'].fillna(value=0)\n",
    "    return train\n",
    "\n",
    "def pretest():\n",
    "    test = predata(15,30)\n",
    "    \n",
    "    test = test.fillna(value=0)\n",
    "    test.drop_duplicates(inplace=True)\n",
    "    return test\n",
    "    \n",
    "test = pretest()    \n",
    "train = pretrain()\n",
    "\n",
    "print train.shape\n",
    "print train.columns\n",
    "print test.shape\n",
    "print test.columns\n",
    "\n",
    "data_p = predata(-6,9)\n",
    "data_p = pd.merge(data_p,addlabel(10,16),on = 'uid',how = 'left')\n",
    "data_p = data_p.fillna(value=0)\n",
    "data_p.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.pop('label')\n",
    "res = pd.DataFrame()\n",
    "res['uid'] = test['uid']\n",
    "import lightgbm as lgb\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = ['rday', 'rtype', 'dtype', 'dtypecnt', 'rtype_0', 'rtype_1', 'rtype_2',\n",
    "       'rtype_3', 'rtype_4', 'rtype_5', 'dtype_0', 'dtype_1', 'dtype_2',\n",
    "       'dtype_3', 'dtype_4', 'dtype_5', 'dtype_6', 'dtype_7', 'dtype_8',\n",
    "       'dtype_9', 'dtype_10', 'dtype_11', 'dtype_12', 'dtype_13',\n",
    "       'dtype_14', 'dtype_15', 'dtype_16', 'dtype_17', 'dtype_18',\n",
    "       'dtype_19', 'vidcnts', 'watch', 'ave_watch', 'atype0cnt',\n",
    "       'page0cnt', 'maxlog', 'lastlog', 'log_gap', 'logsum', 'log0day',\n",
    "       'log1day', 'log2day', 'log3day', 'log4day', 'log5day', 'log6day',\n",
    "       'log7day', 'log8day', 'log9day', 'log10day', 'log11day',\n",
    "       'log12day', 'log13day', 'log14day', 'log15day', 'atype1cnt',\n",
    "       'page1cnt', 'maxp', 'mostp', 'pdaycnt', 'lastp', 'pday_gap',\n",
    "       'sump', 'p0day', 'p1day', 'p2day', 'p3day', 'p4day', 'p5day',\n",
    "       'p6day', 'p7day', 'p8day', 'p9day', 'p10day', 'p11day', 'p12day',\n",
    "       'p13day', 'p14day', 'p15day', 'ave_p', 'atype2cnt', 'page2cnt',\n",
    "       'lasta', 'aday_gap', 'atype3cnt', 'page3cnt', 'atype4cnt',\n",
    "       'page4cnt', 'atype5cnt', 'asum_0', 'asum_1', 'asum_2', 'asum_3',\n",
    "       'asum_4', 'asum_5', 'asum_6', 'asum_7', 'asum_8', 'asum_9',\n",
    "       'asum_10', 'asum_11', 'asum_12', 'asum_13', 'asum_14', 'asum_15',\n",
    "       'act_sum', 'firsta', 'first_gap', 'ave_act', 'adaycnt',\n",
    "       'ave_act_1', 'asum1', 'psum1', 'atype0_ratio', 'atype1_ratio',\n",
    "       'atype2_ratio', 'atype3_ratio', 'atype4_ratio', 'atype5_ratio',\n",
    "       'page0_ratio', 'page1_ratio', 'page2_ratio', 'page3_ratio',\n",
    "       'page4_ratio'\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_1 = train[feature]\n",
    "temp_2 = data_p[feature]\n",
    "temp_3 = test[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = ['dtype', 'page0cnt', 'page3cnt', 'ave_act', 'page1cnt', 'page1cnt', 'atype0cnt',\n",
    "        'page2cnt', 'atype1cnt', 'act_sum', 'rday', 'rtype', 'lastlog', 'logsum', 'maxlog',\n",
    "        'atype2cnt', 'lasta', 'firsta', 'page4cnt', 'asum_15', 'asum_14', 'asum_12', 'asum_13',\n",
    "        'log_gap', 'atype3cnt', 'asum_11', 'asum_10', 'lastp', 'asum_9', 'aday_gap', 'asum_8',\n",
    "        'first_gap', 'asum_5', 'asum_7', 'asum_6', 'pday_gap']\n",
    "\n",
    "for i in range(0,len(cross)):\n",
    "    print i\n",
    "    for j in range(i + 1,len(cross)):\n",
    "        temp_1[str(i) + 'X' + str(j)] = (temp_1[cross[i]]+1) / (temp_1[cross[j]]+1)\n",
    "        temp_2[str(i) + 'X' + str(j)] = (temp_2[cross[i]]+1) / (temp_2[cross[j]]+1)\n",
    "        temp_3[str(i) + 'X' + str(j)] = (temp_3[cross[i]]+1) / (temp_3[cross[j]]+1)\n",
    "        # print cross[i],cross[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lgb = ['rday', 'rtype', 'dtype', 'dtypecnt', 'rtype_0', 'rtype_1', 'rtype_2',\n",
    "       'rtype_3', 'rtype_4', 'rtype_5', 'maxlog', 'lastlog', 'log_gap',\n",
    "       'logsum', 'log0day', 'log1day', 'log2day', 'log3day', 'log4day',\n",
    "       'log5day', 'log6day', 'log7day', 'log8day', 'log9day', 'log10day',\n",
    "       'log11day', 'log12day', 'log13day', 'log14day', 'log15day',\n",
    "       'lastp', 'pday_gap', 'sump', \n",
    "       'p0day', 'p1day', 'p2day', 'p3day', 'p4day', 'p5day', 'p6day', 'p7day',\n",
    "       'p8day', 'p9day', 'p10day', 'p11day', 'p12day', 'p13day',\n",
    "       'p14day', 'p15day', 'lasta', 'aday_gap',  \n",
    "        'act_sum', 'firsta', 'first_gap', 'ave_act', \n",
    "        'asum_9', 'asum_10','asum_11', 'asum_12', 'asum_13', 'asum_14', 'asum_15', \n",
    "        'atype0cnt', 'atype1cnt', 'atype2cnt', 'atype3cnt', 'atype4cnt', 'atype5cnt', \n",
    "        'page0cnt', 'page1cnt', 'page2cnt', 'page3cnt', 'page4cnt', 'vidcnts', 'watch', 'ave_watch',\n",
    "      'dtype_0', 'dtype_1', 'dtype_2','dtype_3', 'dtype_4', 'dtype_5', 'dtype_6', 'dtype_7', 'dtype_8','dtype_9',\n",
    "       'asum_1', 'asum_2', 'asum_3', 'asum_4', 'asum_5','asum_6', 'asum_7', 'asum_8',\n",
    "       'maxp','10X13','0X3','0X10','0X11','0X13',\n",
    "       '0X14','0X6','1X15','0X15','10X29','8X9','4X15',\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, evals_x_0, train_y, evals_y_0 = train_test_split(temp_1[feature_lgb], y, test_size=0.2,\n",
    "\n",
    "                                                      random_state=2018)\n",
    "train_x, train_x1, train_y, train_y1 = train_test_split(train_x, train_y, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "train_x_2, train_x_3, train_y_2, train_y_3 = train_test_split(train_x, train_y, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "train_x_4, train_x_5, train_y_4, train_y_5 = train_test_split(train_x1, train_y1, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "\n",
    "train_x_0 = [evals_x_0,train_x_2,train_x_3,train_x_4,train_x_5]\n",
    "train_y_0 = [evals_y_0,train_y_2,train_y_3,train_y_4,train_y_5]\n",
    "for i in range(0,5):    \n",
    "    evals_x = train_x_0[i]\n",
    "    evals_y = train_y_0[i]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for j in range(0,5):\n",
    "        if j != i:\n",
    "            train_x = pd.concat([train_x,train_x_0[j]])\n",
    "            train_y = pd.concat([train_y,train_y_0[j]])\n",
    "    \n",
    "    train_x = pd.concat([train_x,temp_2[feature_lgb]])\n",
    "    train_y = pd.concat([train_y,data_p['label']])\n",
    "    \n",
    "    print train_x.shape\n",
    "    print evals_x.shape\n",
    "    \n",
    "    print(\"LGB test\")\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        boosting_type='gbdt', num_leaves=55, reg_alpha=0.0, reg_lambda=1,\n",
    "        max_depth=-1, n_estimators=10000, objective='binary',\n",
    "        subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n",
    "        learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs=-1\n",
    "    )\n",
    "    clf.fit(train_x, train_y, eval_set=[(train_x, train_y),(evals_x, evals_y)], \n",
    "            eval_metric={'auc'},early_stopping_rounds=500, categorical_feature=['rtype', ])\n",
    "    \n",
    "    \n",
    "    res['lgb' + str(i)] = clf.predict_proba(temp_3[feature_lgb])[:,1]\n",
    "\n",
    "res['lgb_1'] = (res['lgb0']+res['lgb1']+res['lgb2']+res['lgb3']+res['lgb4'])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, evals_x_0, train_y, evals_y_0 = train_test_split(temp_1[feature_lgb], y, test_size=0.2,\n",
    "\n",
    "                                                      random_state=2018)\n",
    "train_x, train_x1, train_y, train_y1 = train_test_split(train_x, train_y, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "train_x_2, train_x_3, train_y_2, train_y_3 = train_test_split(train_x, train_y, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "train_x_4, train_x_5, train_y_4, train_y_5 = train_test_split(train_x1, train_y1, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "\n",
    "train_x_0 = [evals_x_0,train_x_2,train_x_3,train_x_4,train_x_5]\n",
    "train_y_0 = [evals_y_0,train_y_2,train_y_3,train_y_4,train_y_5]\n",
    "for i in range(0,5):    \n",
    "    evals_x = train_x_0[i]\n",
    "    evals_y = train_y_0[i]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for j in range(0,5):\n",
    "        if j != i:\n",
    "            train_x = pd.concat([train_x,train_x_0[j]])\n",
    "            train_y = pd.concat([train_y,train_y_0[j]])\n",
    "    \n",
    "    train_x = pd.concat([train_x,temp_2[feature_lgb]])\n",
    "    train_y = pd.concat([train_y,data_p['label']])\n",
    "    \n",
    "    print train_x.shape\n",
    "    print evals_x.shape\n",
    "    \n",
    "    print(\"XGB test\")\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    devals = xgb.DMatrix(evals_x, label=evals_y)\n",
    "    dtest = xgb.DMatrix(temp_3[feature_lgb])\n",
    "    params = {'booster': 'gbtree',\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'max_depth': 7,\n",
    "        'lambda': 0,\n",
    "        'subsample': 1.0,\n",
    "        'colsample_bytree': 0.75,\n",
    "        'min_child_weight': 50,\n",
    "        'eta': 0.01,\n",
    "        'seed': 2018,\n",
    "        # 'nthread': 4,\n",
    "        'silent': 1}\n",
    "    watchlist = [(dtrain, 'train'),(devals,'evals')]\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=10000, evals=watchlist,early_stopping_rounds=300)\n",
    "    \n",
    "    res['xgb' + str(i)] = bst.predict(dtest)\n",
    "    \n",
    "res['xgb_1'] = (res['xgb0']+res['xgb1']+res['xgb2']+res['xgb3']+res['xgb4'])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, evals_x_0, train_y, evals_y_0 = train_test_split(temp_1[feature_lgb], y, test_size=0.2,\n",
    "\n",
    "                                                      random_state=2018)\n",
    "train_x, train_x1, train_y, train_y1 = train_test_split(train_x, train_y, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "train_x_2, train_x_3, train_y_2, train_y_3 = train_test_split(train_x, train_y, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "train_x_4, train_x_5, train_y_4, train_y_5 = train_test_split(train_x1, train_y1, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "\n",
    "train_x_0 = [evals_x_0,train_x_2,train_x_3,train_x_4,train_x_5]\n",
    "train_y_0 = [evals_y_0,train_y_2,train_y_3,train_y_4,train_y_5]\n",
    "for i in range(0,5):    \n",
    "    evals_x = train_x_0[i]\n",
    "    evals_y = train_y_0[i]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for j in range(0,5):\n",
    "        if j != i:\n",
    "            train_x = pd.concat([train_x,train_x_0[j]])\n",
    "            train_y = pd.concat([train_y,train_y_0[j]])\n",
    "    \n",
    "    train_x = pd.concat([train_x,temp_2[feature_lgb]])\n",
    "    train_y = pd.concat([train_y,data_p['label']])\n",
    "    # train_y = np.array(train_y.tolist())\n",
    "    print train_x.shape\n",
    "    print evals_x.shape\n",
    "    # print train_y[0]\n",
    "    print(\"LGB test\")\n",
    "    params = {\n",
    "        'boosting':'dart',\n",
    "        'colsample_bytree':0.7,\n",
    "        'learning_rate':0.01,\n",
    "        'metric':'auc',\n",
    "        'min_child_samples':50,\n",
    "        'num_leaves':55,\n",
    "        'objective':'regression',\n",
    "        'reg_alpha':0,\n",
    "        'reg_lambda':1,\n",
    "        'seed':2018,\n",
    "        'silent':1,\n",
    "        'subsample':0.7,\n",
    "        'verbose':1\n",
    "    }\n",
    "    train_part = lgb.Dataset(train_x,label=train_y[0])\n",
    "    evals = lgb.Dataset(evals_x,label=evals_y)\n",
    "    bst = lgb.train(params,train_part, \n",
    "                  num_boost_round=20000, valid_sets=[train_part,evals], \n",
    "                  valid_names=['train','evals'], fobj=None,feval=None,feature_name='auto', \n",
    "                  categorical_feature='auto', early_stopping_rounds=1000,\n",
    "                  evals_result=None, verbose_eval=True, learning_rates=None, \n",
    "                  keep_training_booster=False, callbacks=None)\n",
    "    \n",
    "    \n",
    "    res['reg' + str(i)] = bst.predict(temp_3[feature_lgb])\n",
    "    \n",
    "res['reg_1'] = (res['reg0']+res['reg1']+res['reg2']+res['reg3']+res['reg4'])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lgb = ['rday', 'rtype', 'dtype', 'rtype_0', 'rtype_1', 'rtype_2',\n",
    "       'rtype_3', 'rtype_4', 'rtype_5', 'maxlog', 'lastlog', \n",
    "       'logsum', 'log0day', 'log1day', 'log2day', 'log3day', 'log4day',\n",
    "       'log5day', 'log6day', 'log7day', 'log8day', 'log9day', 'log10day',\n",
    "       'log11day', 'log12day', 'log13day', 'log14day', 'log15day',\n",
    "       'lastp', 'pday_gap', 'sump', \n",
    "       'p0day', 'p1day', 'p2day', 'p3day', 'p4day', 'p5day', 'p6day', 'p7day',\n",
    "       'p8day', 'p9day', 'p10day', 'p11day', 'p12day', 'p13day',\n",
    "       'p14day', 'p15day', 'lasta', 'aday_gap',  \n",
    "        'act_sum', 'firsta', 'first_gap', 'ave_act', \n",
    "        'asum_9', 'asum_10','asum_11', 'asum_12', 'asum_13', 'asum_14', 'asum_15', \n",
    "        'atype0cnt', 'atype1cnt', 'atype2cnt', 'atype3cnt', 'atype4cnt', 'atype5cnt', \n",
    "        'page0cnt', 'page1cnt', 'page2cnt', 'page3cnt', 'page4cnt', 'vidcnts', 'watch', 'ave_watch',\n",
    "      'dtype_0', 'dtype_1', 'dtype_2','dtype_3', 'dtype_4', 'dtype_5', 'dtype_6', 'dtype_7', 'dtype_8','dtype_9',\n",
    "       'asum_1', 'asum_2', 'asum_3', 'asum_4', 'asum_5','asum_6', 'asum_7', 'asum_8',\n",
    "       'maxp','10X13','0X3','0X10','0X11','0X13',\n",
    "       '0X14','0X6','1X15','0X15','10X29','8X9','4X15',\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, evals_x_0, train_y, evals_y_0 = train_test_split(temp_1[feature_lgb], y, test_size=0.2,\n",
    "\n",
    "                                                      random_state=2018)\n",
    "train_x, train_x1, train_y, train_y1 = train_test_split(train_x, train_y, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "train_x_2, train_x_3, train_y_2, train_y_3 = train_test_split(train_x, train_y, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "train_x_4, train_x_5, train_y_4, train_y_5 = train_test_split(train_x1, train_y1, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "\n",
    "train_x_0 = [evals_x_0,train_x_2,train_x_3,train_x_4,train_x_5]\n",
    "train_y_0 = [evals_y_0,train_y_2,train_y_3,train_y_4,train_y_5]\n",
    "for i in range(0,5):    \n",
    "    evals_x = train_x_0[i]\n",
    "    evals_y = train_y_0[i]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for j in range(0,5):\n",
    "        if j != i:\n",
    "            train_x = pd.concat([train_x,train_x_0[j]])\n",
    "            train_y = pd.concat([train_y,train_y_0[j]])\n",
    "    \n",
    "    train_x = pd.concat([train_x,temp_2[feature_lgb]])\n",
    "    train_y = pd.concat([train_y,data_p['label']])\n",
    "    \n",
    "    print train_x.shape\n",
    "    print evals_x.shape\n",
    "    \n",
    "    print(\"LGB test\")\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        boosting_type='gbdt', num_leaves=55, reg_alpha=0.0, reg_lambda=1,\n",
    "        max_depth=-1, n_estimators=10000, objective='binary',\n",
    "        subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n",
    "        learning_rate=0.01, min_child_weight=50, random_state=2018, n_jobs=-1\n",
    "    )\n",
    "    clf.fit(train_x, train_y, eval_set=[(train_x, train_y),(evals_x, evals_y)], \n",
    "            eval_metric={'auc'},early_stopping_rounds=500, categorical_feature=['rtype', ])\n",
    "    \n",
    "    \n",
    "    res['lgb' + str(i)] = clf.predict_proba(temp_3[feature_lgb])[:,1]\n",
    "\n",
    "res['lgb_2'] = (res['lgb0']+res['lgb1']+res['lgb2']+res['lgb3']+res['lgb4'])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, evals_x_0, train_y, evals_y_0 = train_test_split(temp_1[feature_lgb], y, test_size=0.2,\n",
    "\n",
    "                                                      random_state=2018)\n",
    "train_x, train_x1, train_y, train_y1 = train_test_split(train_x, train_y, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "train_x_2, train_x_3, train_y_2, train_y_3 = train_test_split(train_x, train_y, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "train_x_4, train_x_5, train_y_4, train_y_5 = train_test_split(train_x1, train_y1, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "\n",
    "train_x_0 = [evals_x_0,train_x_2,train_x_3,train_x_4,train_x_5]\n",
    "train_y_0 = [evals_y_0,train_y_2,train_y_3,train_y_4,train_y_5]\n",
    "for i in range(0,5):    \n",
    "    evals_x = train_x_0[i]\n",
    "    evals_y = train_y_0[i]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for j in range(0,5):\n",
    "        if j != i:\n",
    "            train_x = pd.concat([train_x,train_x_0[j]])\n",
    "            train_y = pd.concat([train_y,train_y_0[j]])\n",
    "    \n",
    "    train_x = pd.concat([train_x,temp_2[feature_lgb]])\n",
    "    train_y = pd.concat([train_y,data_p['label']])\n",
    "    \n",
    "    print train_x.shape\n",
    "    print evals_x.shape\n",
    "    \n",
    "    print(\"XGB test\")\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    devals = xgb.DMatrix(evals_x, label=evals_y)\n",
    "    dtest = xgb.DMatrix(temp_3[feature_lgb])\n",
    "    params = {'booster': 'gbtree',\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'max_depth': 7,\n",
    "        'lambda': 0,\n",
    "        'subsample': 1.0,\n",
    "        'colsample_bytree': 0.75,\n",
    "        'min_child_weight': 50,\n",
    "        'eta': 0.01,\n",
    "        'seed': 2018,\n",
    "        # 'nthread': 4,\n",
    "        'silent': 1}\n",
    "    watchlist = [(dtrain, 'train'),(devals,'evals')]\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=10000, evals=watchlist,early_stopping_rounds=300)\n",
    "    \n",
    "    res['xgb' + str(i)] = bst.predict(dtest)\n",
    "    \n",
    "res['xgb_2'] = (res['xgb0']+res['xgb1']+res['xgb2']+res['xgb3']+res['xgb4'])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, evals_x_0, train_y, evals_y_0 = train_test_split(temp_1[feature_lgb], y, test_size=0.2,\n",
    "\n",
    "                                                      random_state=2018)\n",
    "train_x, train_x1, train_y, train_y1 = train_test_split(train_x, train_y, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "train_x_2, train_x_3, train_y_2, train_y_3 = train_test_split(train_x, train_y, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "train_x_4, train_x_5, train_y_4, train_y_5 = train_test_split(train_x1, train_y1, test_size=0.5,\n",
    "                                                      random_state=2018)\n",
    "\n",
    "train_x_0 = [evals_x_0,train_x_2,train_x_3,train_x_4,train_x_5]\n",
    "train_y_0 = [evals_y_0,train_y_2,train_y_3,train_y_4,train_y_5]\n",
    "for i in range(0,5):    \n",
    "    evals_x = train_x_0[i]\n",
    "    evals_y = train_y_0[i]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for j in range(0,5):\n",
    "        if j != i:\n",
    "            train_x = pd.concat([train_x,train_x_0[j]])\n",
    "            train_y = pd.concat([train_y,train_y_0[j]])\n",
    "    \n",
    "    train_x = pd.concat([train_x,temp_2[feature_lgb]])\n",
    "    train_y = pd.concat([train_y,data_p['label']])\n",
    "    # train_y = np.array(train_y.tolist())\n",
    "    print train_x.shape\n",
    "    print evals_x.shape\n",
    "    # print train_y[0]\n",
    "    print(\"LGB test\")\n",
    "    params = {\n",
    "        'boosting':'dart',\n",
    "        'colsample_bytree':0.7,\n",
    "        'learning_rate':0.01,\n",
    "        'metric':'auc',\n",
    "        'min_child_samples':50,\n",
    "        'num_leaves':55,\n",
    "        'objective':'regression',\n",
    "        'reg_alpha':0,\n",
    "        'reg_lambda':1,\n",
    "        'seed':2018,\n",
    "        'silent':1,\n",
    "        'subsample':0.7,\n",
    "        'verbose':1\n",
    "    }\n",
    "    train_part = lgb.Dataset(train_x,label=train_y[0])\n",
    "    evals = lgb.Dataset(evals_x,label=evals_y)\n",
    "    bst = lgb.train(params,train_part, \n",
    "                  num_boost_round=20000, valid_sets=[train_part,evals], \n",
    "                  valid_names=['train','evals'], fobj=None,feval=None,feature_name='auto', \n",
    "                  categorical_feature='auto', early_stopping_rounds=1000,\n",
    "                  evals_result=None, verbose_eval=True, learning_rates=None, \n",
    "                  keep_training_booster=False, callbacks=None)\n",
    "    \n",
    "    \n",
    "    res['reg' + str(i)] = bst.predict(temp_3[feature_lgb])\n",
    "    \n",
    "res['reg_2'] = (res['reg0']+res['reg1']+res['reg2']+res['reg3']+res['reg4'])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback, ModelCheckpoint\n",
    "# from keras import callbacks\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_nn = ['rday', 'rtype', 'dtype', 'rtype_0', 'rtype_1', 'rtype_2',\n",
    "       'rtype_3', 'rtype_4', 'rtype_5', 'dtype_0', 'dtype_1', 'dtype_2',\n",
    "       'dtype_3', 'dtype_4', 'dtype_5', 'dtype_6', 'dtype_7', 'dtype_8',\n",
    "       'dtype_9', 'dtype_10', 'dtype_11', 'dtype_12', 'dtype_13',\n",
    "       'dtype_14', 'dtype_15', 'dtype_16', 'dtype_17', 'dtype_18',\n",
    "       'dtype_19', 'vidcnts', 'watch', 'ave_watch', 'atype0cnt',\n",
    "       'page0cnt', 'maxlog', 'lastlog', 'log_gap', 'logsum', 'log0day',\n",
    "       'log1day', 'log2day', 'log3day', 'log4day', 'log5day', 'log6day',\n",
    "       'log7day', 'log8day', 'log9day', 'log10day', 'log11day',\n",
    "       'log12day', 'log13day', 'log14day', 'log15day', 'atype1cnt',\n",
    "       'page1cnt', 'maxp', 'mostp', 'pdaycnt', 'lastp', 'pday_gap',\n",
    "       'sump', 'p0day', 'p1day', 'p2day', 'p3day', 'p4day', 'p5day',\n",
    "       'p6day', 'p7day', 'p8day', 'p9day', 'p10day', 'p11day', 'p12day',\n",
    "       'p13day', 'p14day', 'p15day', 'ave_p', 'atype2cnt', 'page2cnt',\n",
    "       'lasta', 'aday_gap', 'atype3cnt', 'page3cnt', 'atype4cnt',\n",
    "       'page4cnt', 'atype5cnt', 'asum_0', 'asum_1', 'asum_2', 'asum_3',\n",
    "       'asum_4', 'asum_5', 'asum_6', 'asum_7', 'asum_8', 'asum_9',\n",
    "       'asum_10', 'asum_11', 'asum_12', 'asum_13', 'asum_14', 'asum_15',\n",
    "       'act_sum', 'firsta', 'first_gap', 'ave_act', 'adaycnt',\n",
    "       'ave_act_1', 'asum1', 'psum1', 'atype0_ratio', 'atype1_ratio',\n",
    "       'atype2_ratio', 'atype3_ratio', 'atype4_ratio', 'atype5_ratio',\n",
    "       'page0_ratio', 'page1_ratio', 'page2_ratio', 'page3_ratio',\n",
    "       'page4_ratio',\n",
    "      'maxp','10X13','0X3','0X10','0X11','0X13','dtypecnt', \n",
    "      ]\n",
    "train_x, evals_x, train_y, evals_y = train_test_split(temp_1[feature_nn], y, test_size=0.2,\n",
    "                                                      random_state=2018)\n",
    "\n",
    "train_x = pd.concat([train_x,temp_2[feature_nn]])\n",
    "train_y = pd.concat([train_y,data_p['label']])\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.x_val,self.y_val = validation_data\n",
    "    def on_epoch_end(self, epoch, log={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.x_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print('\\n ROC_AUC - epoch:%d - score:%.6f \\n' % (epoch+1, score))\n",
    "\n",
    "max_features = len(feature_nn)\n",
    "model = Sequential()\n",
    "# model.add(Embedding(max_features, output_dim=32))train_x.shape[1],train_x.shape[2]\n",
    "model.add(Dense(128,input_shape=(max_features,)))#\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.05))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.05))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "\n",
    "callbacks = [\n",
    "#     Histories(),\n",
    "    RocAucEvaluation(validation_data=(evals_x,evals_y), interval=1),\n",
    "    EarlyStopping(monitor='loss', patience=3, verbose=0),\n",
    "    ReduceLROnPlateau(monitor='loss', factor=0.1, patience=1, verbose=1, epsilon=1e-4, mode='min'),\n",
    "    # ModelCheckpoint('lstm_model', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    ]\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=256 * 4, epochs=100,\n",
    "         validation_data=(evals_x,evals_y), class_weight = {0:1,1:1},\n",
    "          callbacks=callbacks)\n",
    "\n",
    "res['nn_1'] = model.predict(test[feature_nn], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['score']=(res['xgb_1']*0.4+res['lgb_1']*0.6)*0.42\\\n",
    "            +(res['nn_1']*0.1+res['xgb_2']*0.45+res['lgb_2']*0.45)*0.2 \\\n",
    "            +(res['reg_1']*0.6+res['reg_2']*0.4)*0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = test.loc[:,['uid','rday','lastlog','rtype']]\n",
    "temp['score'] = res['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = temp.groupby('score')['rday','rtype','lastlog'].size().reset_index().rename(columns = {0:'scorecnt'})\n",
    "temp = pd.merge(temp,temp1,on = 'score',how = 'right')\n",
    "temp.sort_index(axis = 0,ascending = False,by = 'scorecnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.loc[((temp.scorecnt >= 50) & (temp.score <= 0.05 ) & (temp.rday <= 15 )),'score'] = 0\n",
    "temp[['uid','score']].to_csv('mysubmission.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nv -O kesci_submit https://cdn.kesci.com/submit_tool/v1/kesci_submit&&chmod +x kesci_submit\n",
    "!./kesci_submit -token 0e0bfea8c2a86f26 -file mysubmission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "res = pd.read_csv('/home/kesci/work/lgb+xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('/home/kesci/work/lgb+xgb.csv',index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = test.loc[:,['uid','rday','lastlog','rtype']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "# import seaborn as sns\n",
    "# color = sns.color_palette()\n",
    "# temp.drop_duplicates(inplace=True)\n",
    "# temp['score'] = res['score']\n",
    "# temp.loc[(temp.lastlog == 9)&(temp.score > 0.0468785)&(temp.score < 0.0468795),\\\n",
    "#         'score'] = 0\n",
    "# \n",
    "# temp.loc[((temp.rday + temp.lastlog == 16) & (temp.rday >= 7 )) | (temp.lastlog == 0),'score'] = 0\n",
    "temp.loc[((temp.scorecnt >= 50) & (temp.score <= 0.05 ) & (temp.rday <= 15 )),'score'] = 0\n",
    "# temp.loc[((temp.scorecnt >= 10) & (temp.score <= 0.087 ) & (temp.rday <= 15 )),'score'] = 0\n",
    "# temp.loc[((temp.scorecnt >= 50) & (temp.score <= 0.087 )),'score'] = 0\n",
    "# temp.loc[(temp.scorecnt >= 10000),'score'] = 0\n",
    "# cnt = temp.groupby('rday').score.size()\n",
    "# plt.figure(figsize=(12,6))\n",
    "# sns.barplot(cnt.index, cnt.values, alpha=0.8, color=color[0])\n",
    "# plt.ylabel('CVR', fontsize=12)\n",
    "# plt.xlabel('day', fontsize=12)\n",
    "# plt.show()\n",
    "temp[['uid','score']].to_csv('mysubmission.csv', index=False, header=False)\n",
    "temp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = temp.groupby('score')['rday','rtype','lastlog'].size().reset_index().rename(columns = {0:'scorecnt'})\n",
    "temp = pd.merge(temp,temp1,on = 'score',how = 'right')\n",
    "temp.sort_index(axis = 0,ascending = False,by = 'scorecnt')\n",
    "# temp.describe()\n",
    "# temp.pop('uid')\n",
    "# temp.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = reguser.loc[:,['uid','rday']]\n",
    "cnt = temp1.groupby('rday').size()\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(cnt.index, cnt.values, alpha=0.8, color=color[0])\n",
    "plt.ylabel('CVR', fontsize=12)\n",
    "plt.xlabel('day', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
